{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Find_best_CNN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-r_tI-3IhsP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0d61b9c-51b8-4625-8c33-9c962c132add"
      },
      "source": [
        "#Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, AvgPool2D, BatchNormalization, Reshape\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import LearningRateScheduler\n",
        "import matplotlib.pyplot as plt\n",
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqXBg80wrF-C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#set seed to make the result reproducible \n",
        "from numpy.random import seed\n",
        "seed(1)\n",
        "from tensorflow import set_random_seed\n",
        "set_random_seed(2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuF0n9ZQFyH2",
        "colab_type": "text"
      },
      "source": [
        "## Load the cifar dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNeH_uKFIled",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "download data\n",
        "(x_train, y_train),(x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "#labels=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
        "\n",
        "#labels 0,1,8,9 --->  Transport\n",
        "#labels 2,3,4,5,6,7 ----> Animals\n",
        "#For training data combine the 10 classes to two classes,0 for transport and 1 for animals\n",
        "\n",
        "y_train[np.where(np.isin(y_train,[0,1,8,9]))] = 0\n",
        "y_train[np.where(np.isin(y_train,[2,3,4,5,6,7]))] = 1\n",
        "\n",
        "#two classes labels\n",
        "labels = ['Transport','Animal']\n",
        "\n",
        "\n",
        "#For test data combine the 10 classes to two classes,0 for transport and 1 for animals\n",
        "y_test[np.where(np.isin(y_test,[0,1,8,9]))] = 0\n",
        "y_test[np.where(np.isin(y_test,[2,3,4,5,6,7]))] = 1\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bk9Q8IcPGCbO",
        "colab_type": "text"
      },
      "source": [
        "## Normalize the inputs and reshape the labels"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ptMZDWdInrO",
        "colab_type": "code",
        "outputId": "c4bb3cc4-1af6-468d-e312-40e7d9572a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train[:40000])\n",
        "x_train = (x_train[:40000]  / 255.0)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40000, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5V3BRCWIp6k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#use learning rate scheduler\n",
        "annealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x, verbose=0)\n",
        "styles=[':','-.','--','-',':','-.','--','-',':','-.','--','-']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp_HciV2GVLG",
        "colab_type": "text"
      },
      "source": [
        "## How many pairs of convolution-subsampling should we use?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z90d3mTIsAO",
        "colab_type": "code",
        "outputId": "c38cddaa-2e77-4aad-a47e-f541541a93ea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        }
      },
      "source": [
        "# BUILD the CNN\n",
        "nets = 3\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(3):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(16,kernel_size=5,padding='same',activation='relu',\n",
        "            input_shape=(32,32,3)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    if j>0:\n",
        "        model[j].add(Conv2D(32,kernel_size=5,padding='same',activation='relu'))\n",
        "        model[j].add(MaxPool2D())\n",
        "    if j>1:\n",
        "        model[j].add(Conv2D(64,kernel_size=5,padding='same',activation='relu'))\n",
        "        model[j].add(MaxPool2D(padding='same'))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(2, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0720 00:11:43.521624 140308054640512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0720 00:11:43.524361 140308054640512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0720 00:11:43.529478 140308054640512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0720 00:11:43.553358 140308054640512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0720 00:11:43.584976 140308054640512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0720 00:11:43.607852 140308054640512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwwXUY71I7PR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "9e0eabfb-f80d-45e2-9a35-74a1d7923753"
      },
      "source": [
        "#Compile and train\n",
        "# Divide training data to training and validation datasets\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(x_train, y_train, test_size = 0.2)\n",
        "# Train the CNN\n",
        "history = [0] * nets\n",
        "names = [\"(C-P)x1\",\"(C-P)x2\",\"(C-P)x3\"]\n",
        "epochs = 20\n",
        "for j in range(nets):\n",
        "    history[j] = model[j].fit(X_train2,Y_train2, batch_size=128, epochs = epochs, \n",
        "        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0719 00:18:44.426304 140086052452224 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0719 00:18:44.481204 140086052452224 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CNN (C-P)x1: Epochs=20, Train accuracy=0.99203, Validation accuracy=0.92488\n",
            "CNN (C-P)x2: Epochs=20, Train accuracy=0.99675, Validation accuracy=0.93037\n",
            "CNN (C-P)x3: Epochs=20, Train accuracy=0.99950, Validation accuracy=0.92888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IshGs9txA-QO",
        "colab_type": "text"
      },
      "source": [
        "## How many Feature maps for each layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCTw_SMqMdi9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "8750b0e1-b9cf-4962-809f-80e2d64fba94"
      },
      "source": [
        "# Build the CNN\n",
        "nets = 6\n",
        "model = [0] *nets\n",
        "for j in range(6):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(j*8+8,kernel_size=5,activation='relu',input_shape=(32,32,3)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Conv2D(j*16+16,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(256, activation='relu'))\n",
        "    model[j].add(Dense(2, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0720 00:11:48.951632 140308054640512 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t2SQaA-vMfKD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "8a67ac9c-0254-4c28-bfcb-683aa8128e7e"
      },
      "source": [
        "# Divide train dataset to training and validation datasets\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(x_train, y_train, test_size = 0.2)\n",
        "# Train the CNN\n",
        "history = [0] * nets\n",
        "names = [\"8 maps\",\"16 maps\",\"24 maps\",\"32 maps\",\"48 maps\",\"64 maps\"]\n",
        "epochs = 30\n",
        "for j in range(nets):\n",
        "    history[j] = model[j].fit(X_train2,Y_train2, batch_size=128, epochs = epochs, \n",
        "        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN 8 maps: Epochs=30, Train accuracy=0.99956, Validation accuracy=0.94050\n",
            "CNN 16 maps: Epochs=30, Train accuracy=0.99997, Validation accuracy=0.96413\n",
            "CNN 24 maps: Epochs=30, Train accuracy=1.00000, Validation accuracy=0.96800\n",
            "CNN 32 maps: Epochs=30, Train accuracy=1.00000, Validation accuracy=0.96913\n",
            "CNN 48 maps: Epochs=30, Train accuracy=1.00000, Validation accuracy=0.96788\n",
            "CNN 64 maps: Epochs=30, Train accuracy=0.99997, Validation accuracy=0.96888\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lid8sjNfBT4o",
        "colab_type": "text"
      },
      "source": [
        "## How many units used for the dense layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaLV00DqOvq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Build the CNN\n",
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(32,32,3)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Flatten())\n",
        "    if j>0:\n",
        "        model[j].add(Dense(2**(j+4), activation='relu'))\n",
        "    model[j].add(Dense(2, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXfJLfTMOv6X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "c2de5c43-715c-4ee2-ad85-090aab78b314"
      },
      "source": [
        "# Divid the train data into training and validation datasets\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(x_train, y_train, test_size = 0.2)\n",
        "# Train the CNN\n",
        "history = [0] * nets\n",
        "names = [\"0N\",\"32N\",\"64N\",\"128N\",\"256N\",\"512N\",\"1024N\",\"2048N\"]\n",
        "epochs = 30\n",
        "for j in range(nets):\n",
        "    history[j] = model[j].fit(X_train2,Y_train2, batch_size=128, epochs = epochs, \n",
        "        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN 0N: Epochs=30, Train accuracy=0.98859, Validation accuracy=0.94312\n",
            "CNN 32N: Epochs=30, Train accuracy=0.99994, Validation accuracy=0.95337\n",
            "CNN 64N: Epochs=30, Train accuracy=0.99994, Validation accuracy=0.95562\n",
            "CNN 128N: Epochs=30, Train accuracy=0.99975, Validation accuracy=0.93088\n",
            "CNN 256N: Epochs=30, Train accuracy=0.99738, Validation accuracy=0.92737\n",
            "CNN 512N: Epochs=30, Train accuracy=0.99997, Validation accuracy=0.92900\n",
            "CNN 1024N: Epochs=30, Train accuracy=1.00000, Validation accuracy=0.93375\n",
            "CNN 2048N: Epochs=30, Train accuracy=0.99997, Validation accuracy=0.92637\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPdPGnZ5CF5n",
        "colab_type": "text"
      },
      "source": [
        "## How much dropout to use after each Conv-pool layer?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adEIxaWMPk-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "420fb84f-b024-4fee-8588-875e0bfd4ba7"
      },
      "source": [
        "# Build the CNN\n",
        "nets = 8\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(8):\n",
        "    model[j] = Sequential()\n",
        "    model[j].add(Conv2D(32,kernel_size=5,activation='relu',input_shape=(32,32,3)))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Conv2D(64,kernel_size=5,activation='relu'))\n",
        "    model[j].add(MaxPool2D())\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dense(64, activation='relu'))\n",
        "    model[j].add(Dropout(j*0.1))\n",
        "    model[j].add(Dense(2, activation='softmax'))\n",
        "    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0720 00:11:59.509644 140308054640512 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0720 00:11:59.518380 140308054640512 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0720 00:12:00.287987 140308054640512 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0720 00:12:00.319332 140308054640512 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0720 00:12:00.353201 140308054640512 nn_ops.py:4224] Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0720 00:12:00.428774 140308054640512 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "W0720 00:12:00.457487 140308054640512 nn_ops.py:4224] Large dropout rate: 0.7 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBx8igh7PlKb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "fcd8e7c1-5983-453d-8cd7-17c276a919e0"
      },
      "source": [
        "# Divide the training data into training and validation datasets\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(x_train, y_train, test_size = 0.2)\n",
        "# train the network\n",
        "history = [0] * nets\n",
        "names = [\"D=0\",\"D=0.1\",\"D=0.2\",\"D=0.3\",\"D=0.4\",\"D=0.5\",\"D=0.6\",\"D=0.7\"]\n",
        "epochs = 30\n",
        "for j in range(nets):\n",
        "    history[j] = model[j].fit(X_train2,Y_train2, batch_size=128, epochs = epochs, \n",
        "        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN D=0: Epochs=30, Train accuracy=0.99866, Validation accuracy=0.92963\n",
            "CNN D=0.1: Epochs=30, Train accuracy=0.97891, Validation accuracy=0.93800\n",
            "CNN D=0.2: Epochs=30, Train accuracy=0.96862, Validation accuracy=0.94050\n",
            "CNN D=0.3: Epochs=30, Train accuracy=0.95778, Validation accuracy=0.93763\n",
            "CNN D=0.4: Epochs=30, Train accuracy=0.94644, Validation accuracy=0.93988\n",
            "CNN D=0.5: Epochs=30, Train accuracy=0.93581, Validation accuracy=0.93712\n",
            "CNN D=0.6: Epochs=30, Train accuracy=0.92266, Validation accuracy=0.92875\n",
            "CNN D=0.7: Epochs=30, Train accuracy=0.91087, Validation accuracy=0.91625\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlWMXL7KCZ_V",
        "colab_type": "text"
      },
      "source": [
        "# Choose between different models "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1r4IRsrTVDa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "j=4\n",
        "model[j] = Sequential()\n",
        "\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu',input_shape=(32,32,3)))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(32,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=3,activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Conv2D(64,kernel_size=5,strides=2,padding='same',activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "\n",
        "model[j].add(Flatten())\n",
        "model[j].add(Dense(64, activation='relu'))\n",
        "model[j].add(BatchNormalization())\n",
        "model[j].add(Dropout(0.4))\n",
        "model[j].add(Dense(2, activation='softmax'))\n",
        "\n",
        "model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8ODtvTSTVS2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "outputId": "6b950be0-03b9-4873-84e1-5fa9b7313a3b"
      },
      "source": [
        "# Divide data into training and validation datasets\n",
        "X_train2, X_val2, Y_train2, Y_val2 = train_test_split(x_train, y_train, test_size = 0.2)\n",
        "#Traing 4 different models with diffrent features\n",
        "history = [0] * nets\n",
        "names = [\"basic\",\"32C3-32C3\",\"32C5S2\",\"both+BN\",\"both+BN+DA\"]\n",
        "epochs = 50\n",
        "for j in range(nets-1):\n",
        "    history[j] = model[j].fit(X_train2,Y_train2, batch_size=128, epochs = epochs,  \n",
        "        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "    print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "        names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))\n",
        "    \n",
        "# Use data augmentation to create more training data\n",
        "datagen = ImageDataGenerator(\n",
        "        rotation_range=10,  \n",
        "        zoom_range = 0.1,  \n",
        "        width_shift_range=0.1, \n",
        "        height_shift_range=0.1)\n",
        "# Train the last netwrok with all the features added\n",
        "j = nets-1\n",
        "history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=128), \n",
        "    epochs = epochs, steps_per_epoch = X_train2.shape[0]//128,\n",
        "    validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n",
        "print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n",
        "    names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN basic: Epochs=50, Train accuracy=0.99803, Validation accuracy=0.92650\n",
            "CNN 32C3-32C3: Epochs=50, Train accuracy=0.99906, Validation accuracy=0.96788\n",
            "CNN 32C5S2: Epochs=50, Train accuracy=0.99275, Validation accuracy=0.96725\n",
            "CNN both+BN: Epochs=50, Train accuracy=0.98059, Validation accuracy=0.95813\n",
            "CNN both+BN+DA: Epochs=50, Train accuracy=0.99591, Validation accuracy=0.95788\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-057ac65044be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     history[j] = model[j].fit(X_train2,Y_train2, batch_size=128, epochs = epochs,  \n\u001b[0;32m----> 8\u001b[0;31m         validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=0)\n\u001b[0m\u001b[1;32m      9\u001b[0m     print(\"CNN {0}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n\u001b[1;32m     10\u001b[0m         names[j],epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}